{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Statistical Tests, Hypothesis Testing, Power Analysis, and Bootstrapping Lab\n",
    "\n",
    "Week 2 | 5.1\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Hypothesis testing including performing the z-test and t-test.\n",
    "- Calculation of confidence intervals.\n",
    "- Power analysis and sample size calculation.\n",
    "- Bootstrapping for measuring uncertainty of statistics on samples.\n",
    "\n",
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Be able to code basic python syntax\n",
    "- Be comfortable using packages\n",
    "\n",
    "### INSTRUCTOR PREP\n",
    "*Before this lesson, instructors will need to:*\n",
    "- Review Learning Objectives \n",
    "- Review problems discussed during lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hr = pd.read_csv(\"/Users/lana/Desktop/DSI-SF-5/datasets/hr_analytics/HR_comma_sep.csv\")\n",
    "star = pd.read_csv('/Users/lana/Desktop/DSI-SF-5/datasets/starcraft/SkillCraft1_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Clean datatypes and clean the Starcraft data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GameID                    int64\n",
       "LeagueIndex               int64\n",
       "Age                      object\n",
       "HoursPerWeek             object\n",
       "TotalHours               object\n",
       "APM                     float64\n",
       "SelectByHotkeys         float64\n",
       "AssignToHotkeys         float64\n",
       "UniqueHotkeys             int64\n",
       "MinimapAttacks          float64\n",
       "MinimapRightClicks      float64\n",
       "NumberOfPACs            float64\n",
       "GapBetweenPACs          float64\n",
       "ActionLatency           float64\n",
       "ActionsInPAC            float64\n",
       "TotalMapExplored          int64\n",
       "WorkersMade             float64\n",
       "UniqueUnitsMade           int64\n",
       "ComplexUnitsMade        float64\n",
       "ComplexAbilitiesUsed    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>LeagueIndex</th>\n",
       "      <th>Age</th>\n",
       "      <th>HoursPerWeek</th>\n",
       "      <th>TotalHours</th>\n",
       "      <th>APM</th>\n",
       "      <th>SelectByHotkeys</th>\n",
       "      <th>AssignToHotkeys</th>\n",
       "      <th>UniqueHotkeys</th>\n",
       "      <th>MinimapAttacks</th>\n",
       "      <th>MinimapRightClicks</th>\n",
       "      <th>NumberOfPACs</th>\n",
       "      <th>GapBetweenPACs</th>\n",
       "      <th>ActionLatency</th>\n",
       "      <th>ActionsInPAC</th>\n",
       "      <th>TotalMapExplored</th>\n",
       "      <th>WorkersMade</th>\n",
       "      <th>UniqueUnitsMade</th>\n",
       "      <th>ComplexUnitsMade</th>\n",
       "      <th>ComplexAbilitiesUsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>143.7180</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>32.6677</td>\n",
       "      <td>40.8673</td>\n",
       "      <td>4.7508</td>\n",
       "      <td>28</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>129.2322</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>32.9194</td>\n",
       "      <td>42.3454</td>\n",
       "      <td>4.8434</td>\n",
       "      <td>22</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>69.9612</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>44.6475</td>\n",
       "      <td>75.3548</td>\n",
       "      <td>4.0430</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>107.6016</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>29.2203</td>\n",
       "      <td>53.7352</td>\n",
       "      <td>4.9155</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>122.8908</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>22.6885</td>\n",
       "      <td>62.0813</td>\n",
       "      <td>9.3740</td>\n",
       "      <td>15</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GameID  LeagueIndex   Age  HoursPerWeek  TotalHours       APM  \\\n",
       "0      52            5  27.0          10.0      3000.0  143.7180   \n",
       "1      55            5  23.0          10.0      5000.0  129.2322   \n",
       "2      56            4  30.0          10.0       200.0   69.9612   \n",
       "3      57            3  19.0          20.0       400.0  107.6016   \n",
       "4      58            3  32.0          10.0       500.0  122.8908   \n",
       "\n",
       "   SelectByHotkeys  AssignToHotkeys  UniqueHotkeys  MinimapAttacks  \\\n",
       "0         0.003515         0.000220              7        0.000110   \n",
       "1         0.003304         0.000259              4        0.000294   \n",
       "2         0.001101         0.000336              4        0.000294   \n",
       "3         0.001034         0.000213              1        0.000053   \n",
       "4         0.001136         0.000327              2        0.000000   \n",
       "\n",
       "   MinimapRightClicks  NumberOfPACs  GapBetweenPACs  ActionLatency  \\\n",
       "0            0.000392      0.004849         32.6677        40.8673   \n",
       "1            0.000432      0.004307         32.9194        42.3454   \n",
       "2            0.000461      0.002926         44.6475        75.3548   \n",
       "3            0.000543      0.003783         29.2203        53.7352   \n",
       "4            0.001329      0.002368         22.6885        62.0813   \n",
       "\n",
       "   ActionsInPAC  TotalMapExplored  WorkersMade  UniqueUnitsMade  \\\n",
       "0        4.7508                28     0.001397                6   \n",
       "1        4.8434                22     0.001194                5   \n",
       "2        4.0430                22     0.000745                6   \n",
       "3        4.9155                19     0.000426                7   \n",
       "4        9.3740                15     0.001174                4   \n",
       "\n",
       "   ComplexUnitsMade  ComplexAbilitiesUsed  \n",
       "0               0.0              0.000000  \n",
       "1               0.0              0.000208  \n",
       "2               0.0              0.000189  \n",
       "3               0.0              0.000384  \n",
       "4               0.0              0.000019  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_float_conv(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "star.Age = star.Age.map(try_float_conv)\n",
    "star.HoursPerWeek = star.HoursPerWeek.map(try_float_conv)\n",
    "star.TotalHours = star.TotalHours.map(try_float_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10.,   20.,    6.,    8.,   42.,   14.,   24.,   16.,    4.,\n",
       "         12.,   30.,   28.,   70.,    2.,   56.,   36.,   40.,   18.,\n",
       "         96.,   50.,  168.,   48.,   84.,    0.,   72.,  112.,   90.,\n",
       "         32.,   98.,  140.,   nan,   80.,   60.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star.HoursPerWeek.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "star = star.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3338, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Examine the distribution of age\n",
    "\n",
    "Is this normally distributed? \n",
    "\n",
    "There are multiple formal tests for normality. One of the popular ones is the [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test), which will test if a distribution is not normally distributed (the null hypothesis in this case is that the distribution _is_ normally distributed.)\n",
    "\n",
    "The test is already built into scipy under `scipy.stats.shapiro`. After plotting the age distribution, use this to formally test if the distribution is normally distributed.\n",
    "\n",
    "[Shapiro-Wilk test Scipy API](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lana/anaconda/envs/dsi/lib/python2.7/site-packages/scipy/stats/morestats.py:1330: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02689504623413086, 0.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.shapiro(star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the Age distribution normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The mean age and central limit theorem.\n",
    "\n",
    "Take 500 random samples of size 20 from the age values in the Starcraft data and calculate the mean age for each. Take another 500 random samples but this time take 100 at a time and calculate the mean for each. \n",
    "\n",
    "Plot both of these distributions of means on the same plot. Why do the distributions differ in shape, and how is this difference quantified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use np.random.choice for sampling\n",
    "# sample with replacement - and explain why\n",
    "# use a for loop to take both samples \n",
    "# save the mean of each batch (each 20 or 100) to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot both samples on the same graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between these distributions is quantified by the difference in standard error of the measurement, which defines their standard deviation. \n",
    "\n",
    "The standard error of the mean here will be the standard deviation of the `Age` distribution divided by the square root of the number of people in the sample. (Typically you would need to approximate the standard deviation of the age population by replacing it with the standard deviation of the age measurements in your sample.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do these distributions of means conform to the normal distribution? Test this with the shapiro test as you did before for the age distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.shapiro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculating a z-statistic\n",
    "\n",
    "#### Standardization and the Z statistic\n",
    "\n",
    "How can we use the CLT in practice? \n",
    "\n",
    "We can transform data into the same space, or rather, standardize them into a normal distribution. We use the Z-score to do this:\n",
    "\n",
    "Z= (x-μ)/σ\n",
    "\n",
    "Conversion to z-scores place distinct populations on the same metric. This is a technique to deal with exploring new data, outliers, and feature engineering.\n",
    "\n",
    "Let's say we're interested in the average of a certain set of data, We can use the normal\n",
    "distribution even if we do not know the distribution of the population from which the sample\n",
    "was drawn, by calculating the z-statistic (applying the concept of standardization to the mean)\n",
    "\n",
    "Z= (x-μ)/(σ/√n)\n",
    "\n",
    "The standard error of the mean σ/√n is the standard deviation of the sampling distribution\n",
    "of the sample mean:\n",
    "* It describes the mean of multiple members of a population.\n",
    "* It is always smaller than the standard deviation.\n",
    "* The larger our sample size is, the smaller is the standard error and less we would expect our sample mean to differ from the population mean\n",
    "\n",
    "\n",
    "**Create** a normal distribution object using `scipy.stats.norm` with mean 0 and standard deviation 1. Plot the probability density across a range of x values from -4 to 4 using the `.pdf` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and don't forget to label your plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the central limit theorem, we know that as n approaches infinity the distribution of sample means becomes normally distributed. \n",
    "\n",
    "We can leverage this fact to calculate the _uncertainty_ of our measurement of the mean. This is done with a **confidence interval**. Confidence intervals are unfortunately confusing in their interpretation. A 95% confidence interval does NOT mean \"there is a 95% probability that the true mean falls in this range.\" Remember that in frequentist statistics there is no probability associated with the true parameter/statistic: it is fixed at a specific value.\n",
    "\n",
    "The correct interpretation of a 95% confidence interval around the mean is instead:\n",
    "\n",
    "> 95% of repeated experiments that calculate a confidence interval with this threshold (alpha = 0.05) will contain the true mean.\n",
    "\n",
    "Which means that 95% of hypothetical confidence intervals calculated on samples the same size from the population will contain the true mean.\n",
    "\n",
    "Note here that the probability is associated with the data, not with the true parameter being estimated. This is the critical difference in frequentist statistics (vs. bayesian); our data has a probability of occuring, but there is no probability of values for the true parameter.\n",
    "\n",
    "This is confusing and unfortunately there really isn't any way around this. You have to change the way that you intuitively think about probability when doing frequentist statistics. Perhaps an easier way to think about the confidence interval is in terms of the Type I error:\n",
    "\n",
    "> With a 95% confidence interval, if the true value lies outside of the calculated confidence interval then this is the result of a sampling error which had a 5% probability of occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculating the confidence interval of a mean\n",
    "\n",
    "\n",
    "#### Confidence Intervals\n",
    "\n",
    "How do we use our knowledge of means, standard errors, and the empirical rule to analyze data? We can produce confidence intervals to describe ranges our statistics, like mean, fall into.\n",
    "\n",
    "Specifically, The confidence interval is a range of values around the mean for which if we drew an infinite number of samples of the same size from the same population, x% of the time the true population mean would be included in the confidence interval calculated from the samples. It gives us the information about the precision of a point estimate such as the sample mean.\n",
    "\n",
    "**Three things affect the width of a confidence interval**\n",
    "\n",
    "* Confidence Level: usually 95%.\n",
    "* Variability: if there is more variation in a population, each sample taken will fluctuate more and wider the confidence interval. The variability of the population is estimated using the standard deviation from the sample.\n",
    "* Sample size: without lowering the confidence level, the sample size can control the width of a confidence interval, having an inverse square root relationship to it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Leveraging the CLT we can calculate a confidence interval for our sample mean. The CLT states that our sample mean falls on a normal distribution of sample means defined with standard deviation:\n",
    "\n",
    "### $$ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} $$\n",
    "\n",
    "Where $\\sigma$ is the overall population standard deviation. In reality we typically do not have the population standard deviation and so would use the sample standard deviation $s$ in its place. This is conventionally considered a valid approach when $n > 30$.\n",
    "\n",
    "### $$  \\sigma_{\\bar{x}} = \\frac{s}{\\sqrt{n}} $$\n",
    "\n",
    "Where $s$ is the sample standard deviation.\n",
    "\n",
    "Say we want to calculate a 95% confidence interval around our sample mean. **First, calculate the standard deviations around the unit-variance normal distribution that contains 95% of the area.**\n",
    "\n",
    "The `.ppf` function that is part of the normal distribution object will be useful for this. It converts percentiles into values (on the x-axis, in this case standard deviations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal.ppf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "upper = normal.ppf(1. - alpha/2.)\n",
    "lower = normal.ppf(alpha/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper, lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the calculated area under the normal curve that makes up the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the lower and upper bounds, use the calculated standard error of the mean to convert these z-statistic values into the bounds around our sample mean.\n",
    "\n",
    "The confidence interval bounds are calculated like so:\n",
    "\n",
    "### $$ [\\; \\bar{x} + Z_{\\alpha/2}\\frac{s}{\\sqrt{n}},\\; \\bar{x} + Z_{1-\\alpha/2} \\frac{s}{\\sqrt{n}} ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the sample mean and 99% confidence interval for age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "upper = normal.ppf(1. - alpha/2.)\n",
    "lower = normal.ppf(alpha/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the confidence interval bounds here\n",
    "# print age_lower, mean_age, age_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The t-statistic vs. z-statistic\n",
    "\n",
    "[The student t-distribution](https://en.wikipedia.org/wiki/Student's_t-distribution) is essentially a more conservative version of the normal distribution, for when sample sizes are small.\n",
    "\n",
    "As $n$ grows, the t-distribution approaches the normal distribution. When $n$ is small, the tails of the distribution are \"fatter\" and thus more density falls further from the mean. This in turn means that confidence intervals calculated on the t-distribution are more conservative.\n",
    "\n",
    "You can create the t-distribution using `scipy.stats.t`. You will need to specify the degrees of freedom when instantiating this object, which should be $n - 1$. For example, `tdist = stats.t(49)` will create a t-distribution for sample size $n = 50$\n",
    "\n",
    "**Plot the pdf of the t-distribution for sample size of 5 and sample size of 100 on the same plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use stats.t \n",
    "stats.t?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the confidence interval for the sample mean of age from the two t-distributions.\n",
    "\n",
    "(This will be as if that measured sample mean was taken from 5 observations vs. 100 observations as opposed to the fact that it was actually taken from over 3000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "upper = tdist5.ppf(1. - alpha/2.)\n",
    "lower = tdist5.ppf(alpha/2.)\n",
    "\n",
    "# calculate the confidence interval here 5 obs\n",
    "# print age_lower, mean_age, age_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "upper = tdist100.ppf(1. - alpha/2.)\n",
    "lower = tdist100.ppf(alpha/2.)\n",
    "\n",
    "# calculate the confidence interval for the 5 obs yes \n",
    "# print age_lower, mean_age, age_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hypothesis testing with HR data\n",
    "\n",
    "Load in the HR data. Say we want to test whether the `satisfaction_level` is greater for management employees with high salary (job type is defined in the `sales` column) vs. management employees with low salary.\n",
    "\n",
    ">$H_0$: There is no difference in satisfaction level between high and low salary management.\n",
    "\n",
    ">$H_1$: There is a difference in satisfaction level between high and low salary management.\n",
    "\n",
    "**Plot the distribution of satisfaction level for the two groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# suggestion: create a mask to get a dataframe with high paid management \n",
    "# suggestion: create a mask to get a dataframe with low paid management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out the means of each distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Set up the hypothesis test\n",
    "\n",
    "We are going examine whether there is a significant difference between the mean satisfaction rate between the groups.\n",
    "\n",
    "**Calculate the difference in mean satisfaction rate between groups.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have two different groups, we need to pool the variance in order to find the error around our measurement of the mean. We can be conservative and not assume that the two groups have equal variance. \n",
    "\n",
    "The formula for the pooled standard error of the mean (not assuming equal variances) is:\n",
    "\n",
    "### $$ s_p = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}} $$\n",
    "\n",
    "**Calculate the pooled standard error of the mean.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's find the t-statistic for our difference between means. Remember that our null hypothesis $H_0$ states that there is no difference between means. Therefore we want to find where our difference in mean satisfaction would fall on that t-distribution.\n",
    "\n",
    "The t-distribution of sample means will have a mean 0 and standard deviation 1. We therefore need to have our sample mean in terms of standard deviations:\n",
    "\n",
    "### $$ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} $$\n",
    "\n",
    "**Standardize our difference between mean satsifaction by dividing by the standard error of the mean.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up the null hypothesis t-distribution.**\n",
    "\n",
    "We can choose a conservative degrees of freedom that is the smaller between $n_1 - 1$ and $n_2 -1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the 95% confidence interval for our sample difference between means.**\n",
    "\n",
    "What does this confidence interval tell us? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the confidence interival here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your obsverstaion about the confidence interval here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Type I error ($\\alpha$),  Type II error ($\\beta$), and Power ($1 - \\beta$)\n",
    "\n",
    "We tend to think a lot about Type I error, or the probability of false positives. It's counterpart, Type II error, is also quite important. Type II error measures the risk of false negatives. \n",
    "\n",
    "Whereas the p-value or $\\alpha$ is the probability that we could have gotten our measured effect by chance due to sampling error, the $\\beta$ is the probability that we _accepted_ the null hypothesis by chance when in fact the alternative hypothesis is true.\n",
    "\n",
    "The more common formulation of this false negative rate is **Power**, which is $1 - \\beta$. You can think of power as the probability of correctly accepting the alternative hypothesis when it is true.\n",
    "\n",
    "---\n",
    "\n",
    "Below is an interactive visualization that helps show the relationship between the $\\alpha$ threshold and the power of the experiment. Decreasing alpha (making the threshold more strict) necessarily reduces our power. Alternatively, the more lenient we are about Type I error rate the more we reduce Type II error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def power_plotter(std, alpha, effect):\n",
    "    unit_norm = stats.norm(0,1)\n",
    "    z_alpha = unit_norm.ppf(1-alpha/2.)\n",
    "    z_power = (effect/std) - z_alpha \n",
    "    \n",
    "    null_dist = stats.norm(0, std)\n",
    "    alt_dist = stats.norm(effect, std)\n",
    "    \n",
    "    power = unit_norm.cdf(z_power)\n",
    "    \n",
    "    z_type1 = null_dist.ppf(1-alpha/2)\n",
    "    \n",
    "    xvals = np.linspace(-5*std, effect+(5*std), 300)\n",
    "\n",
    "    fig, axarr = plt.subplots(2, 1, figsize=(14,6))\n",
    "\n",
    "    axarr[0].plot(xvals, null_dist.pdf(xvals), color='grey', lw=4, label='Null')\n",
    "    axarr[1].plot(xvals, alt_dist.pdf(xvals), color='black', lw=4, label='Alternative')\n",
    "\n",
    "    type1_x = np.linspace(z_type1, effect+(5*std))\n",
    "\n",
    "    axarr[0].axvline(z_type1, lw=3, ls='dashed', color='darkred')\n",
    "    axarr[0].fill_between(type1_x, 0, null_dist.pdf(type1_x), color='darkred', \n",
    "                          alpha=0.5, hatch = '///', label='alpha = {:.2f}'.format(alpha))\n",
    "    \n",
    "    axarr[1].axvline(z_type1, lw=3, ls='dashed', color='steelblue')\n",
    "    axarr[1].fill_between(type1_x, 0, alt_dist.pdf(type1_x), color='steelblue', \n",
    "                          alpha=0.5, hatch = '///', label='power = {:.2f}'.format(power))\n",
    "\n",
    "\n",
    "    axarr[0].set_ylim([0, 0.50])\n",
    "    axarr[1].set_ylim([0, 0.50])\n",
    "    \n",
    "    axarr[0].set_xlim([-5,10])\n",
    "    axarr[1].set_xlim([-5,10])\n",
    "    \n",
    "    axarr[0].legend(loc='upper left', fontsize=14)\n",
    "    axarr[1].legend(loc='upper left', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def power_calcplotter(std_effect=1.0, alpha=0.05, effect=1.0):\n",
    "    power_plotter(std_effect, alpha, effect)\n",
    "    \n",
    "\n",
    "interact(power_calcplotter, \n",
    "         std_effect=FloatSlider(min=0.5, max=3.0, value=1.0, step=0.1),\n",
    "         alpha=FloatSlider(min=0.01, max=0.50, value=0.05, step=0.01),\n",
    "         effect=FloatSlider(min=0.1, max=5.0, value=1.0, step=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Power analysis and sample size calculation\n",
    "\n",
    "It is often the case that we are not just analyzing data already gathered, but instead want to set up an experiment to test a hypothesis we have. The design of experiments is a deep and extensive topic - for now we will just be covering power analysis and sample size calculation.\n",
    "\n",
    "If we are setting up an experiment with a control condition and experimental condition to test a hypothesis, we will want to know how many subjects we should recruit for each group. In order to calculate the sample size we will need to know:\n",
    "\n",
    "1. Our desired Type I error rate ($\\alpha$)\n",
    "2. Our desired Type II error rate ($\\beta$ or more commonly power, $1 - \\beta$)\n",
    "3. The expected size of the effect, or mean difference between groups.\n",
    "4. The expected standard deviation of measurement.\n",
    "\n",
    "Often the expected effect size and the standard deviation of measurement will need to be estimated from prior work or other information.\n",
    "\n",
    "**Sample size calculations are hypothetical.** Because we can't really know until we actually collect the data, these are just guidelines for the setup of the experiment and should therefore be fairly conservative. There is good reason to not exceed the required number of subjects by too much though: there are marginal returns with more subjects, and tests are often costly to run.\n",
    "\n",
    "---\n",
    "\n",
    "With $\\theta$ being a test statistic that is a difference between groups,\n",
    "\n",
    "$SE_{\\theta}$ the standard error of measurement on the test statistic,\n",
    "\n",
    "$\\alpha$ the specified Type I error threshold value (p-value),\n",
    "\n",
    "$Z_{\\alpha}$ the z-statistic for the specified $\\alpha$ (position on the unit-variance normal distribution),\n",
    "\n",
    "The z-statistic for the _power_, which is $1 - \\beta$ or in other words 1 minus the Type II error rate, is calculated:\n",
    "\n",
    "### $$ Z_{power} = \\frac{\\theta}{SE_{\\theta}} - Z_{\\alpha/2} $$\n",
    "\n",
    "This can be re-written with $\\sigma^2$, the (assumed) equal standard deviation of measurements in the two groups, \n",
    "\n",
    "$n_1$, the number of people in group 1,\n",
    "\n",
    "and $r$, the ratio of group 2 to group 1\n",
    "\n",
    "### $$ Z_{power} = \\frac{\\theta}{ \\sqrt{ \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{rn_1} } } - Z_{\\alpha/2} $$\n",
    "\n",
    "If you solve for $n_1$ you get:\n",
    "\n",
    "### $$ n_1 = \\frac{r+1}{r} \\frac{\\sigma^2 (Z_{power} + Z_{\\alpha/2})^2}{\\theta^2} $$\n",
    "\n",
    "If you assume that the two groups will have equal sizes (which is most often the case when preparing a test: the best power for lowest number of subjects is achieved with equal groups), $r = 1$ and the formula reduces to:\n",
    "\n",
    "### $$ n = \\frac{2 \\sigma^2 (Z_{power} + Z_{\\alpha/2})^2}{\\theta^2} $$\n",
    "\n",
    "where $n$ is the number of subjects requred in each group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_calculator(std, desired_power, alpha, effect):\n",
    "    unit_norm = stats.norm(0,1)\n",
    "    z_power = unit_norm.ppf(desired_power)\n",
    "    z_alpha = unit_norm.ppf(1-alpha/2.)\n",
    "    n = np.ceil((2. * std * (z_power + z_alpha)**2) / (effect**2))\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def required_sample_plotter(std=1, desired_power=0.8, alpha=0.05, effect=0.5):\n",
    "    n = n_calculator(std, desired_power, alpha, effect)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    \n",
    "    null_dist = stats.norm(0, std)\n",
    "    alt_dist = stats.norm(effect, std)\n",
    "        \n",
    "    null_means = np.array([np.mean(null_dist.rvs(n)) - np.mean(null_dist.rvs(n)) for i in range(1000)])\n",
    "    alt_means = np.array([np.mean(alt_dist.rvs(n)) - np.mean(null_dist.rvs(n)) for i in range(1000)])\n",
    "    \n",
    "    alpha_null_val = stats.scoreatpercentile(null_means, (1. - (alpha/2))*100)\n",
    "    \n",
    "    pct_alt_above = np.sum(alt_means > alpha_null_val)/1000.\n",
    "    pct_null_above = np.sum(null_means > alpha_null_val)/1000.\n",
    "        \n",
    "    ax = sns.distplot(null_means[null_means < alpha_null_val],\n",
    "                      ax=ax, color='grey', kde=False, hist_kws=dict(alpha=0.2))\n",
    "    ax = sns.distplot(null_means[null_means >= alpha_null_val],\n",
    "                      ax=ax, color='darkred', kde=False, hist_kws=dict(alpha=0.4),\n",
    "                      label='alpha')\n",
    "    \n",
    "    ax = sns.distplot(alt_means[alt_means < alpha_null_val], ax=ax,\n",
    "                      color='steelblue', kde=False, hist_kws=dict(alpha=0.4),\n",
    "                      label='beta or (1 - power)')\n",
    "    ax = sns.distplot(alt_means[alt_means >= alpha_null_val], \n",
    "                      ax=ax, color='grey', kde=False, hist_kws=dict(alpha=0.2))\n",
    "    \n",
    "    ax.axvline(alpha_null_val, lw=3, ls='dashed', color='black')\n",
    "    \n",
    "    n_str = 'N = '+str(int(n))\n",
    "    effect_str = 'effect size = {:.2f}'.format(float(effect)/std)\n",
    "    power_str = 'power = {:.3f}'.format(pct_alt_above)\n",
    "    alpha_str = 'alpha = {:.3f}'.format(pct_null_above)\n",
    "    \n",
    "    ax.annotate('\\n'.join([n_str, effect_str, alpha_str, power_str]),\n",
    "                xy=(0.95, 0.6),\n",
    "                xycoords='axes fraction', horizontalalignment='right', \n",
    "                fontsize=14, weight='bold', color='black')\n",
    "    \n",
    "    ax.legend(loc='upper left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deltas = required_sample_plotter(1, 0.8, 0.05, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping is a powerful resampling method that allows us to estimate practically any statistic and our uncertainty around it directly from the data without needing to make assumptions about the distribution of the statistics.\n",
    "\n",
    "The idea behind bootstrapping is that inferences we can draw from samples of our sample can be applied to our inference about the overall population that the original sample is drawn from. Essentially, bootstrap samples are to our original sample as our original sample is to the overall population.\n",
    "\n",
    "> Infer $\\theta$ of population from sample $\\hat{\\theta}_{s}$ with uncertainty estimated from sample\n",
    "\n",
    "> Infer uncertainty around $\\hat{\\theta}_{s}$ (distribution of parameter) estimated from multiple bootstrap samples\n",
    "\n",
    "Because our sample is assumed to be representative of the overall population (drawn randomly from the population), we can then by extension say that inferences we make comparing our sample to random subsamples are applicable to the overall population. We create these bootstrap samples in the same way that we assume we are drawing our sample from the population.\n",
    "\n",
    "Why and when is bootstrapping useful? In the case we have been looking at so far, the estimation of sample means or the difference between means, it is not as immediately useful because the mean has nice properties. But for some types of statistics like the sample median or the sample correlation, it is hard to measure our uncertainty about the statistics because the standard deviations of the statistics aren't simple to compute.\n",
    "\n",
    "---\n",
    "\n",
    "### Confidence intervals for the median age in Starcraft data\n",
    "\n",
    "Assuming:\n",
    "\n",
    "1. $N$ is large\n",
    "2. The sample of measurements is drawn from a normally distributed population\n",
    "\n",
    "We could calculate the sample error of the median as:\n",
    "\n",
    "### $$ \\text{s.e. median} = 1.2533 \\cdot \\text{s.e. mean} $$\n",
    "\n",
    "But how do we know that the population we are drawing our sample from is normally distributed? Furthermore, why would we want the median as opposed to the mean if we know that the population is normally distributed?\n",
    "\n",
    "Typically we will want the median and a confidence interval around it when we suspect a non-normally distributed population. Let's take the distribution of APM (actions per minute) from our Starcraft data as the overall population.\n",
    "\n",
    "**Plot the distribution of APM from the Starcraft data with a vertical line indicating the median.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a random sample of 50 people and their APM values from the overall Starcraft population.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a 99% confidence interval for the median for your sample computed with the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the sample median and the confidence interval bounds with the histogram of sample APM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using that inappropriate formula, we can use bootstrapping to calculate the confidence interval for our sample median.\n",
    "\n",
    "There are a variety of formulas to measure uncertainty bounds with bootstrapping. We'll use the most straightforward way by computing the median across many random bootstrap samples and then taking the lower and upper percentiles of medians according to our 99% threshold.\n",
    "\n",
    "The pseudocode for this bootstrapping procedure is below. This is an example of **nonparametric bootstrapping**.\n",
    "\n",
    "```\n",
    "for specified number of bootstrap iterations\n",
    "    create a bootstrap sample by randomly selecting observations with replacement from your sample \n",
    "        (same size as sample)\n",
    "    calculate the statistic of interest on bootstrap sample\n",
    "\n",
    "calculate lower and upper percentile bounds of bootstrap statistics according to threshold\n",
    "```\n",
    "\n",
    "**Write a function to calculate a statistic of interest on a specified number of bootstrap iterations from a sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the confidence bounds for 99% confidence interval using the bootstrapping function with 1000 iterations.\n",
    "\n",
    "The `scipy.stats.scoreatpercentile()` function will help you get these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the bootstrap confidence interval bounds vs. the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
